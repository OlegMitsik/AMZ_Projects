{
  "metadata" : {
    "kernelspec" : {
      "display_name" : "Python 2",
      "language" : "python",
      "name" : "python2"
    },
    "language_info" : {
      "file_extension" : ".py",
      "mimetype" : "text/x-python",
      "name" : "python"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 2,
  "cells" : [ {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\n\r\n\r\nimport datetime\r\nfrom s3fs.core import S3FileSystem\r\nfrom pandas.api.types import is_string_dtype\r\nfrom collections import OrderedDict" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df = pd.read_table(\"/tmp/rank2_customers.tmp\")\r\n\r\nprint(\"Shape of  the dataset:\" , df.shape)\r\nprint(\"Number of unique customers:\", df['encrypted_customer_id'].nunique(), \"number of unique ASINs: \", df['asin'].nunique())" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def get_proper_shape(df):\n    \"\"\"function converts to pd datetime and adds recency and get rid of promotion_title and gets rid of different promotion titles\"\"\"\n    df['transaction_datetime_local'] = pd.to_datetime(df['transaction_datetime_local'])\n    df['week_ending'] = pd.to_datetime(df['week_ending'])\n    \n    last_date = df.groupby('encrypted_customer_id')['transaction_datetime_local'].max().reset_index()\n    last_date.columns = ['encrypted_customer_id','last_purchase']\n    merged_df = pd.merge(left = df, right = last_date, left_on = 'encrypted_customer_id', right_on = 'encrypted_customer_id', how = 'left')\n    merged_df['recency'] = (merged_df['transaction_datetime_local'].max() - merged_df['last_purchase']) / np.timedelta64(1,\"D\")\n    \n    merged_df.is_copy = False\n    merged_df['is_promo'] = np.where(merged_df['promotion_title'].isnull(), 'non_promo','promo')\n    \n    temp = merged_df.drop(['promotion_title'],axis = 1)\n    f = temp[~temp.duplicated()]\n    \n    return f" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df_new = get_proper_shape(df)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print(\"Shape of  the dataset:\" , df_new.shape)\nprint(\"Number of unique customers:\", df_new['encrypted_customer_id'].nunique(), \"number of unique ASINs: \", df_new['asin'].nunique())\ndf_new.head()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print(df_new.groupby(\"max_rank\")['encrypted_customer_id'].nunique().sum())\ndf_new.groupby(\"max_rank\")['encrypted_customer_id'].nunique()" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# Some EDA\n\n### What did 1st time customers (who did not make second purchase for at least 30 days purchase on their first transaction? " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print(df_new[(df_new['max_rank'] == 1) & (df_new['rank_by_datetime'] == 1)].shape)\nprint(df_new[(df_new['max_rank'] == 1) & (df_new['rank_by_datetime'] == 1)]['encrypted_customer_id'].nunique())" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print(df_new[(df_new['max_rank'] == 1) & (df_new['rank_by_datetime'] == 1) & (df_new['recency'] > 30)].shape)\nprint(df_new[(df_new['max_rank'] == 1) & (df_new['rank_by_datetime'] == 1) & (df_new['recency'] > 30)]['encrypted_customer_id'].nunique())\nprint(df_new[(df_new['max_rank'] == 1) & (df_new['rank_by_datetime'] == 1) & (df_new['recency'] > 30)]['units'].sum())" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def overall_stat(df, core, metric):\n    \"\"\"calculate high level stat with cumsum and shares\"\"\"\n    a = df.groupby([core])[metric].sum().reset_index().sort_values(by = metric, ascending = False)\n    a['share'] = a[metric] / a[metric].sum()\n    a['cumsum'] = a['share'].cumsum()\n    return a" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "b = overall_stat(df_new[(df_new['max_rank'] == 1) & (df_new['rank_by_datetime'] == 1) & (df_new['recency'] > 30)], 'title_name','units')\nb[0:25]" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# Setup for Classification Problem\n\n## Idea is to make a cutoff window, as an example, 2 weeks after pre last purchase as a target" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def incorporate_pre_last_purchase(df):\n    \"\"\"function to trimp datetime and get for each customer pre last purchase date\"\"\"\n    \n    df['datetime'] = df['transaction_datetime_local'].dt.date\n    df['datetime'] = pd.to_datetime(df['datetime'])\n\n    d = df[['encrypted_customer_id','datetime']]\n    d = d.drop_duplicates()\n    \n    #calculate pre last purchase date with method nth(k)\n    pre_last = d.groupby(\"encrypted_customer_id\")['datetime'].nth(-2).reset_index()\n    pre_last.columns = ['encrypted_customer_id','pre_last_purchase']\n    \n    #merge the datasets\n    m = pd.merge(left = df,right = pre_last, left_on = ['encrypted_customer_id'], right_on = ['encrypted_customer_id'], how = 'left')\n    \n    #get the time difference btw last and pre last time stamp\n    m['difference_btw_pre_last_and_last'] = (m['last_purchase'] - m['pre_last_purchase']) / np.timedelta64(1,\"D\")\n    return m\n    \ndf_newds = incorporate_pre_last_purchase(df_new)\nprint(df_new.shape)\nprint(df_newds.shape)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "#Quick check\ndf_newds[df_newds['encrypted_customer_id'] == 'A23PH2G2990QEF']" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df_newds['difference_btw_pre_last_and_last'].hist(range = (0,100), figsize = (15,9)) " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# Another check\n\nprint(df_newds[(df_newds['difference_btw_pre_last_and_last'].isnull()) & (df_newds['max_rank'] > 1) ].shape)\nprint(df_newds[(df_newds['difference_btw_pre_last_and_last'].isnull()) & (df_newds['max_rank'] == 1) ].shape)\nprint(df_newds[(df_newds['difference_btw_pre_last_and_last'].isnull())].shape)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def add_target_value(df, threshold):\n    df['target'] = np.where(df['difference_btw_pre_last_and_last'].isnull(),0,1)\n    df['target'] = np.where((df['difference_btw_pre_last_and_last'] < threshold) & (df['difference_btw_pre_last_and_last'].notnull()), 1,0)\n    \n    return df" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df_newds = add_target_value(df_newds, 30)\nprint(df_newds.shape)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "#Overall stat\ndef get_sizes(df):\n    a = df.groupby('target')['encrypted_customer_id'].nunique().reset_index()\n    a['share'] = a['encrypted_customer_id'] / a['encrypted_customer_id'].sum()\n    return a\n    \nget_size_of_buckets = get_sizes(df_newds)\nget_size_of_buckets\n\n" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df_newds.tail()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def compile_date_set_for_ml(df):\n    df1 = df.groupby(['encrypted_customer_id'])['units','display_price_wo_tax'].sum().reset_index()\n    df2 = df.groupby(\"encrypted_customer_id\")['recency','target'].max().reset_index()\n    \n    df_merged = pd.merge(left = df1,right = df2,left_on = 'encrypted_customer_id', right_on = 'encrypted_customer_id', how = 'left')\n    \n    return df_merged" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df_ml = compile_date_set_for_ml(df_newds)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print(df_newds['encrypted_customer_id'].nunique())\nprint(df_ml.shape)\ndf_ml['encrypted_customer_id'].nunique()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df_ml.head()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "df_ml.groupby(\"target\").mean()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "X = df_ml[['units','display_price_wo_tax','recency']]\nY = df_ml['target']" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print(X.shape, Y.shape)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# Import libraries and make logistic regression" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.3, random_state = 0)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "print(X_train.shape,y_train.shape)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "logreg = LogisticRegression()\r\nlogreg.fit(X_train, y_train)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "## Predict the classes " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "y_pred = logreg.predict(X_test)\r\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "from sklearn import model_selection\r\nfrom sklearn.model_selection import cross_val_score\r\nkfold = model_selection.KFold(n_splits=10, random_state=7)\r\nmodelCV = LogisticRegression()\r\nscoring = 'accuracy'\r\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\r\nprint(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "#Confusion matrix\r\n\r\nfrom sklearn.metrics import confusion_matrix\r\nconfusion_matrix = confusion_matrix(y_test, y_pred)\r\nprint(confusion_matrix)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# Compute precision, recall etc\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "# ROC Curve\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure(figsize = (15,8))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n\nplt.show()" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "predicted_probabilities = logreg.predict_proba(X_test)\nprint(predicted_probabilities.shape, X_test.shape)\npredicted_probabilities" ]
  } ]
}